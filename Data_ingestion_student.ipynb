{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MapathaMatiba/processing-big-data-predict/blob/main/Data_ingestion_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pSqvcAF0JnI"
      },
      "source": [
        "# Processing Big Data - Data Ingestion\n",
        "© Explore Data Science Academy\n",
        "\n",
        "## Honour Code\n",
        "I {**YOUR NAME**, **YOUR SURNAME**}, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
        "    Non-compliance with the honour code constitutes a material breach of contract.\n",
        "\n",
        "\n",
        "\n",
        "## Context\n",
        "\n",
        "To work constructively with any dataset, one needs to create an ingestion profile to make sure that the data at the source can be readily consumed. For this section of the predict, as the Data Engineer in the team, you will be required to design and implement the ingestion process. For the purposes of the project the AWS cloud storage service, namely, the S3 bucket service will act as your data source. All the data required can be found [here](https://processing-big-data-predict-stocks-data.s3.eu-west-1.amazonaws.com/stocks.zip).\n",
        "\n",
        "<div align=\"center\" style=\"width: 600px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
        "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/data_engineering/transform/predict/DataIngestion.jpg\"\n",
        "     alt=\"Data Ingestion\"\n",
        "     style=\"float: center; padding-bottom=0.5em\"\n",
        "     width=40%/>\n",
        "     <p><em>Figure 1. Data Ingestion</em></p>\n",
        "</div>\n",
        "\n",
        "Your manager, Gnissecorp Atadgib, knowing very well that you've recently completed your Data Engineering qualification, asks you to make use of Apache Spark for the ingestion as well as the rest of the project. His rationale being, that stock market data is generated every day and is quite time-sensitive and would require scalability when deploying to a production environment.\n",
        "\n",
        "## Dataset - US Nasdaq\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<div align=\"center\" style=\"width: 600px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
        "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/data_engineering/transform/predict/Nasdaq.png\"\n",
        "     alt=\"Nasdaq\"\n",
        "     style=\"float: center; padding-bottom=0.5em\"\n",
        "     width=50%/>\n",
        "     <p><em>Figure 2. Nasdaq</em></p>\n",
        "</div>\n",
        "\n",
        "The data that you will be working with is a historical snapshot of market data taken from the Nasdaq electronic market. This dataset contains historical daily prices for all tickers currently trading on Nasdaq. The up-to-date list can be found on their [website](https://www.nasdaq.com/)\n",
        "\n",
        "\n",
        "The provided data contains price data dating back from 02 January 1962 up until 01 April 2020. The data found in the S3 bucket has been stored in the following structure:\n",
        "\n",
        "```\n",
        "     stocks/<Year>/<Month>/<Day>/stocks.csv\n",
        "```\n",
        "Each CSV file for every trading day contains the following details:\n",
        "- **Date** - specifies trading date\n",
        "- **Open** - opening price\n",
        "- **High** - maximum price during the day\n",
        "- **Low** - minimum price during the day\n",
        "- **Close** - close price adjusted for splits\n",
        "- **Adj Close** - close price adjusted for both dividends and splits\n",
        "- **Volume** - the number of shares that changed hands during a given day"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JguMEGNi2mNP",
        "outputId": "f5375222-a83e-48e8-d1cc-025b0455032d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YR5bYwC0JnJ"
      },
      "source": [
        "## Basic initialisation\n",
        "To get you started, let's import some basic Python libraries as well as Spark modules and functions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu_iAYw084ZI",
        "outputId": "712b46d4-81b8-452f-ff3f-243258d4761c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=572b3df57c6bf202287be687b34d0a8b8eb6052c7ea779c726f4a557cae29b20\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xK75UJle0JnK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibLJhi6D0JnK"
      },
      "source": [
        "Remember that we need a `SparkContext` and `SparkSession` to interface with Spark.\n",
        "We will mostly be using the `SparkContext` to interact with RDDs and the `SparkSession` to interface with Python objects.\n",
        "\n",
        "> ℹ️ **Instructions** ℹ️\n",
        ">\n",
        ">Initialise a new **Spark Context** and **Session** that you will use to interface with Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_qZGPnw0JnK",
        "outputId": "63c20467-edf5-4342-f801-1be4c288c313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Session initialized: <pyspark.sql.session.SparkSession object at 0x7cdb4476f850>\n"
          ]
        }
      ],
      "source": [
        "#TODO: Write your code here\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Predict\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Verify the Spark Context and Session are initialized\n",
        "print(\"Spark Session initialized:\", spark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLyVKD-L0JnL"
      },
      "source": [
        "## Investigate dataset schema\n",
        "At this point, it is enough to read in a single file to ascertain the data structure. You will be required to use the information obtained from the small subset to create a data schema. This data schema will be used when reading the entire dataset using Spark.\n",
        "\n",
        "> ℹ️ **Instructions** ℹ️\n",
        ">\n",
        ">Make use of Pandas to read in a single file and investigate the plausible data types to be used when creating a Spark data schema.\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvfZSNPr0JnL",
        "outputId": "7911a855-179f-48af-fba4-e6c9f1b51274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date      Open      High       Low     Close  Adj Close    Volume  \\\n",
            "0  1962-01-02  6.532155  6.556185  6.532155  6.532155   1.536658   55900.0   \n",
            "1  1962-01-02  6.125844  6.160982  6.125844  6.125844   1.414651   59700.0   \n",
            "2  1962-01-02  0.837449  0.837449  0.823045  0.823045   0.145748  352200.0   \n",
            "3  1962-01-02  1.604167  1.619792  1.588542  1.604167   0.136957  163200.0   \n",
            "4  1962-01-02  0.000000  3.296131  3.244048  3.296131   0.051993  105600.0   \n",
            "\n",
            "  stock  \n",
            "0    AA  \n",
            "1  ARNC  \n",
            "2    BA  \n",
            "3   CAT  \n",
            "4   CVX  \n",
            "Date          object\n",
            "Open         float64\n",
            "High         float64\n",
            "Low          float64\n",
            "Close        float64\n",
            "Adj Close    float64\n",
            "Volume       float64\n",
            "stock         object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Correct file path for the CSV file\n",
        "file_path = '/content/drive/My Drive/1962/01/02/stocks.csv'\n",
        "\n",
        "# Read the CSV file using Pandas\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "print(df.head())\n",
        "\n",
        "# Display the data types of each column\n",
        "print(df.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5NPkgaeY0JnM",
        "outputId": "fd80c1ae-2890-4060-faa8-c047dab1d79b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Open        High         Low       Close     Adj Close  \\\n",
            "count  20.000000   20.000000   20.000000   20.000000  2.000000e+01   \n",
            "mean    1.202568   18.146229   17.829750   17.967168  6.248174e+00   \n",
            "std     2.457011   58.093799   56.989958   57.542460  2.597106e+01   \n",
            "min     0.000000    0.096026    0.092908    0.092908  6.281419e-07   \n",
            "25%     0.000000    0.640337    0.623523    0.627279  1.185386e-02   \n",
            "50%     0.000000    2.457961    2.416295    2.450149  1.413525e-01   \n",
            "75%     0.772764    8.051069    7.933194    7.933194  8.265629e-01   \n",
            "max     7.713333  263.125000  258.125000  260.625000  1.165582e+02   \n",
            "\n",
            "             Volume  \n",
            "count  2.000000e+01  \n",
            "mean   4.490900e+05  \n",
            "std    7.027279e+05  \n",
            "min    0.000000e+00  \n",
            "25%    4.445000e+04  \n",
            "50%    1.344000e+05  \n",
            "75%    4.920000e+05  \n",
            "max    2.480300e+06  \n",
            "Date          object\n",
            "Open         float64\n",
            "High         float64\n",
            "Low          float64\n",
            "Close        float64\n",
            "Adj Close    float64\n",
            "Volume       float64\n",
            "stock         object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Display summary statistics to understand the data better\n",
        "print(df.describe())\n",
        "\n",
        "# Display the data types of each column\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jXHN8HG0JnM",
        "outputId": "95c4b01a-2494-4dd3-9cb5-2a756018a167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StructType([StructField('Date', DateType(), True), StructField('Open', FloatType(), True), StructField('High', FloatType(), True), StructField('Low', FloatType(), True), StructField('Close', FloatType(), True), StructField('Adj Close', FloatType(), True), StructField('Volume', LongType(), True)])\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StructType, StructField, DateType, StringType, FloatType, LongType\n",
        "\n",
        "# Define the schema for the stock data\n",
        "schema = StructType([\n",
        "    StructField(\"Date\", DateType(), True),\n",
        "    StructField(\"Open\", FloatType(), True),\n",
        "    StructField(\"High\", FloatType(), True),\n",
        "    StructField(\"Low\", FloatType(), True),\n",
        "    StructField(\"Close\", FloatType(), True),\n",
        "    StructField(\"Adj Close\", FloatType(), True),\n",
        "    StructField(\"Volume\", LongType(), True)\n",
        "])\n",
        "\n",
        "# Display the schema\n",
        "print(schema)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV8c8Oy60JnM"
      },
      "source": [
        "## Read CSV files\n",
        "\n",
        "When working with big data, it is often not tenable to keep processing an entire data batch when you are in the process of development - this can be quite time-consuming. If the data is uniform, it is sufficient to work with a smaller subset to create basic functionality. Your manager has identified the year **1962** to perform the initial testing for data ingestion.\n",
        "\n",
        "> ℹ️ **Instructions** ℹ️\n",
        ">\n",
        ">Read in the data for **1962** using a data schema that purely uses string data types. You will be required to convert to the appropriate data types at a later stage.\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiFFfJpf0JnM",
        "outputId": "22e1b265-eceb-41d9-c5c8-8bcbde0fd669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: string (nullable = true)\n",
            " |-- High: string (nullable = true)\n",
            " |-- Low: string (nullable = true)\n",
            " |-- Close: string (nullable = true)\n",
            " |-- Adj Close: string (nullable = true)\n",
            " |-- Volume: string (nullable = true)\n",
            "\n",
            "+----+----+----+---+-----+---------+------+\n",
            "|Date|Open|High|Low|Close|Adj Close|Volume|\n",
            "+----+----+----+---+-----+---------+------+\n",
            "+----+----+----+---+-----+---------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#TODO: Write your code here\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "# Define the schema with all fields as StringType\n",
        "schema = StructType([\n",
        "    StructField(\"Date\", StringType(), True),\n",
        "    StructField(\"Open\", StringType(), True),\n",
        "    StructField(\"High\", StringType(), True),\n",
        "    StructField(\"Low\", StringType(), True),\n",
        "    StructField(\"Close\", StringType(), True),\n",
        "    StructField(\"Adj Close\", StringType(), True),\n",
        "    StructField(\"Volume\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Step 4: Reading the CSV File\n",
        "# Define the file path\n",
        "file_path = \"/content/drive/My Drive/1962\"  # Update this with your actual file path\n",
        "\n",
        "# Read the CSV file using the defined schema\n",
        "df_1962 = spark.read.csv(file_path, header=True, schema=schema)\n",
        "\n",
        "# Step 5: Verifying the Data\n",
        "# Print the schema to verify data types\n",
        "df_1962.printSchema()\n",
        "\n",
        "# Show a sample of the data\n",
        "df_1962.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9DF0DQa0JnM"
      },
      "source": [
        "## Update column names\n",
        "To make the data easier to work with, you will need to make a few changes:\n",
        "1. Column headers should all be in lowercase; and\n",
        "2. Whitespaces should be replaced with underscores.\n",
        "\n",
        "\n",
        "> ℹ️ **Instructions** ℹ️\n",
        ">\n",
        ">Make sure that the column headers are all in lowercase and that any whitespaces are replaced with underscores.\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvnO8_MF0JnM",
        "outputId": "e6c77f26-eb80-4cd6-d614-e0fca8fb8876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         date                open                high                 low  \\\n",
            "0  1962-09-21    5.60699987411499    5.60699987411499   5.446800231933594   \n",
            "1  1962-09-21   5.247376441955566   5.247376441955566   5.118534564971924   \n",
            "2  1962-09-21  0.6502057909965515  0.6563786268234253  0.6378600597381592   \n",
            "3  1962-09-21             1.40625             1.40625            1.390625   \n",
            "4  1962-09-21                 0.0  3.5714285373687744   3.497023820877075   \n",
            "\n",
            "                close            adj_close    volume stock source_file  \n",
            "0    5.45481014251709   1.3031203746795654   25900.0    AA  stocks.csv  \n",
            "1   5.130247592926025   1.2030487060546875   27700.0  ARNC  stocks.csv  \n",
            "2  0.6419752836227417  0.11746639013290405  959600.0    BA  stocks.csv  \n",
            "3             1.40625   0.1225220113992691  230400.0   CAT  stocks.csv  \n",
            "4   3.497023820877075  0.05664148554205895  192000.0   CVX  stocks.csv  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5106 entries, 0 to 5105\n",
            "Data columns (total 9 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   date         5106 non-null   object\n",
            " 1   open         5106 non-null   object\n",
            " 2   high         5106 non-null   object\n",
            " 3   low          5084 non-null   object\n",
            " 4   close        5106 non-null   object\n",
            " 5   adj_close    5106 non-null   object\n",
            " 6   volume       5085 non-null   object\n",
            " 7   stock        5106 non-null   object\n",
            " 8   source_file  5106 non-null   object\n",
            "dtypes: object(9)\n",
            "memory usage: 359.1+ KB\n",
            "None\n",
            "\n",
            "Updated column names:\n",
            "['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume', 'stock', 'source_file']\n"
          ]
        }
      ],
      "source": [
        "#TODO: Write your code here\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Set the root directory for 1962 data\n",
        "root_dir = \"/content/drive/My Drive/1962\"\n",
        "\n",
        "# Create an empty list to store all DataFrames\n",
        "all_dfs = []\n",
        "\n",
        "# Function to clean column names\n",
        "def clean_column_names(df):\n",
        "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
        "    return df\n",
        "\n",
        "# Walk through the directory structure\n",
        "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "    for filename in filenames:\n",
        "        if filename.endswith('.csv'):\n",
        "            file_path = os.path.join(dirpath, filename)\n",
        "\n",
        "            # Read the CSV file as a whole\n",
        "            df = pd.read_csv(file_path, dtype=str)\n",
        "\n",
        "            # Clean column names for the DataFrame\n",
        "            df = clean_column_names(df)\n",
        "\n",
        "            # Add a column to identify the source file\n",
        "            df['source_file'] = filename\n",
        "\n",
        "            # Append to the list of all DataFrames\n",
        "            all_dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "df_1962 = pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "# Print the first few rows and info about the DataFrame\n",
        "print(df_1962.head())\n",
        "print(df_1962.info())\n",
        "\n",
        "# Print column names to verify the changes\n",
        "print(\"\\nUpdated column names:\")\n",
        "print(df_1962.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y81Nbvh00JnN"
      },
      "source": [
        "## Null Values\n",
        "Null values often represent missing pieces of data. It is always good to know where your null values lie - so you can quickly identify and remedy any issues stemming from these.\n",
        "\n",
        "> ℹ️ **Instructions** ℹ️\n",
        ">\n",
        ">Write code to count the number of null values found in each column.\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n_tZubN0JnN",
        "outputId": "cc261664-b920-4cd6-9a87-ea67c2cefe71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         date                open                high                 low  \\\n",
            "0  1962-09-21    5.60699987411499    5.60699987411499   5.446800231933594   \n",
            "1  1962-09-21   5.247376441955566   5.247376441955566   5.118534564971924   \n",
            "2  1962-09-21  0.6502057909965515  0.6563786268234253  0.6378600597381592   \n",
            "3  1962-09-21             1.40625             1.40625            1.390625   \n",
            "4  1962-09-21                 0.0  3.5714285373687744   3.497023820877075   \n",
            "\n",
            "                close            adj_close    volume stock source_file  \n",
            "0    5.45481014251709   1.3031203746795654   25900.0    AA  stocks.csv  \n",
            "1   5.130247592926025   1.2030487060546875   27700.0  ARNC  stocks.csv  \n",
            "2  0.6419752836227417  0.11746639013290405  959600.0    BA  stocks.csv  \n",
            "3             1.40625   0.1225220113992691  230400.0   CAT  stocks.csv  \n",
            "4   3.497023820877075  0.05664148554205895  192000.0   CVX  stocks.csv  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5106 entries, 0 to 5105\n",
            "Data columns (total 9 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   date         5106 non-null   object\n",
            " 1   open         5106 non-null   object\n",
            " 2   high         5106 non-null   object\n",
            " 3   low          5084 non-null   object\n",
            " 4   close        5106 non-null   object\n",
            " 5   adj_close    5106 non-null   object\n",
            " 6   volume       5085 non-null   object\n",
            " 7   stock        5106 non-null   object\n",
            " 8   source_file  5106 non-null   object\n",
            "dtypes: object(9)\n",
            "memory usage: 359.1+ KB\n",
            "None\n",
            "\n",
            "Updated column names:\n",
            "['date', 'open', 'high', 'low', 'close', 'adj_close', 'volume', 'stock', 'source_file']\n",
            "\n",
            "Null value counts for each column:\n",
            "date            0\n",
            "open            0\n",
            "high            0\n",
            "low            22\n",
            "close           0\n",
            "adj_close       0\n",
            "volume         21\n",
            "stock           0\n",
            "source_file     0\n",
            "dtype: int64\n",
            "\n",
            "Percentage of null values in each column:\n",
            "date           0.000000\n",
            "open           0.000000\n",
            "high           0.000000\n",
            "low            0.430866\n",
            "close          0.000000\n",
            "adj_close      0.000000\n",
            "volume         0.411281\n",
            "stock          0.000000\n",
            "source_file    0.000000\n",
            "dtype: float64\n",
            "\n",
            "Columns containing null values:\n",
            "['low', 'volume']\n"
          ]
        }
      ],
      "source": [
        "#TODO: Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Set the root directory for 1962 data\n",
        "root_dir = \"/content/drive/My Drive/1962\"\n",
        "\n",
        "# Create an empty list to store all DataFrames\n",
        "all_dfs = []\n",
        "\n",
        "# Function to clean column names\n",
        "def clean_column_names(df):\n",
        "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
        "    return df\n",
        "\n",
        "# Walk through the directory structure\n",
        "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "    for filename in filenames:\n",
        "        if filename.endswith('.csv'):\n",
        "            file_path = os.path.join(dirpath, filename)\n",
        "\n",
        "            # Read the CSV file as a whole\n",
        "            df = pd.read_csv(file_path, dtype=str)\n",
        "\n",
        "            # Clean column names\n",
        "            df = clean_column_names(df)\n",
        "\n",
        "            # Add a column to identify the source file\n",
        "            df['source_file'] = filename\n",
        "\n",
        "            # Append to the list of all DataFrames\n",
        "            all_dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "df_1962 = pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "# Print the first few rows and info about the DataFrame\n",
        "print(df_1962.head())\n",
        "print(df_1962.info())\n",
        "\n",
        "# Print column names to verify the changes\n",
        "print(\"\\nUpdated column names:\")\n",
        "print(df_1962.columns.tolist())\n",
        "\n",
        "# Count null values in each column\n",
        "null_counts = df_1962.isnull().sum()\n",
        "\n",
        "# Print the count of null values for each column\n",
        "print(\"\\nNull value counts for each column:\")\n",
        "print(null_counts)\n",
        "\n",
        "# Calculate and print the percentage of null values in each column\n",
        "null_percentages = (null_counts / len(df_1962)) * 100\n",
        "print(\"\\nPercentage of null values in each column:\")\n",
        "print(null_percentages)\n",
        "\n",
        "# Identify columns with null values\n",
        "columns_with_nulls = null_counts[null_counts > 0].index.tolist()\n",
        "print(\"\\nColumns containing null values:\")\n",
        "print(columns_with_nulls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asGR8KRA0JnN"
      },
      "source": [
        "## Data type conversion - The final data schema\n",
        "\n",
        "Now that we have identified the number of missing values in the data set, we'll move on to convert our data schema to the required data types.\n",
        "\n",
        "> ℹ️ **Instructions** ℹ️\n",
        ">\n",
        ">Use typecasting to convert the string data types in your current data schema to more appropriate data types.\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TerQuuY0JnN",
        "outputId": "13b87243-f909-4b79-d582-a535b623d943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        date      open      high       low     close            adj_close  \\\n",
            "0 1962-09-21  5.607000  5.607000  5.446800  5.454810   1.3031203746795654   \n",
            "1 1962-09-21  5.247376  5.247376  5.118535  5.130248   1.2030487060546875   \n",
            "2 1962-09-21  0.650206  0.656379  0.637860  0.641975  0.11746639013290405   \n",
            "3 1962-09-21  1.406250  1.406250  1.390625  1.406250   0.1225220113992691   \n",
            "4 1962-09-21  0.000000  3.571429  3.497024  3.497024  0.05664148554205895   \n",
            "\n",
            "     volume stock source_file  \n",
            "0   25900.0    AA  stocks.csv  \n",
            "1   27700.0  ARNC  stocks.csv  \n",
            "2  959600.0    BA  stocks.csv  \n",
            "3  230400.0   CAT  stocks.csv  \n",
            "4  192000.0   CVX  stocks.csv  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5106 entries, 0 to 5105\n",
            "Data columns (total 9 columns):\n",
            " #   Column       Non-Null Count  Dtype         \n",
            "---  ------       --------------  -----         \n",
            " 0   date         5106 non-null   datetime64[ns]\n",
            " 1   open         5106 non-null   float64       \n",
            " 2   high         5106 non-null   float64       \n",
            " 3   low          5064 non-null   float64       \n",
            " 4   close        5106 non-null   float64       \n",
            " 5   adj_close    5106 non-null   object        \n",
            " 6   volume       5085 non-null   float64       \n",
            " 7   stock        5106 non-null   object        \n",
            " 8   source_file  5106 non-null   category      \n",
            "dtypes: category(1), datetime64[ns](1), float64(5), object(2)\n",
            "memory usage: 324.3+ KB\n",
            "None\n",
            "\n",
            "Column names and data types:\n",
            "date           datetime64[ns]\n",
            "open                  float64\n",
            "high                  float64\n",
            "low                   float64\n",
            "close                 float64\n",
            "adj_close              object\n",
            "volume                float64\n",
            "stock                  object\n",
            "source_file          category\n",
            "dtype: object\n",
            "\n",
            "Null value counts for each column:\n",
            "date            0\n",
            "open            0\n",
            "high            0\n",
            "low            42\n",
            "close           0\n",
            "adj_close       0\n",
            "volume         21\n",
            "stock           0\n",
            "source_file     0\n",
            "dtype: int64\n",
            "\n",
            "Percentage of null values in each column:\n",
            "date           0.000000\n",
            "open           0.000000\n",
            "high           0.000000\n",
            "low            0.822562\n",
            "close          0.000000\n",
            "adj_close      0.000000\n",
            "volume         0.411281\n",
            "stock          0.000000\n",
            "source_file    0.000000\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "#TODO: Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Set the root directory for 1962 data\n",
        "root_dir = \"/content/drive/My Drive/1962\"\n",
        "\n",
        "# Create an empty list to store all DataFrames\n",
        "all_dfs = []\n",
        "\n",
        "# Function to clean column names\n",
        "def clean_column_names(df):\n",
        "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
        "    return df\n",
        "\n",
        "# Function to convert data types\n",
        "def convert_dtypes(df):\n",
        "    # Define the data types for each column\n",
        "    dtypes = {\n",
        "        'date': 'datetime64[ns]',\n",
        "        'symbol': 'category',\n",
        "        'open': 'float64',\n",
        "        'high': 'float64',\n",
        "        'low': 'float64',\n",
        "        'close': 'float64',\n",
        "        'volume': 'int64',\n",
        "        'source_file': 'category'\n",
        "    }\n",
        "\n",
        "    # Convert each column to its appropriate data type\n",
        "    for col, dtype in dtypes.items():\n",
        "        if col in df.columns:\n",
        "            if dtype == 'datetime64[ns]':\n",
        "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "            elif dtype in ['float64', 'int64']:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            else:\n",
        "                df[col] = df[col].astype(dtype)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Walk through the directory structure\n",
        "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "    for filename in filenames:\n",
        "        if filename.endswith('.csv'):\n",
        "            file_path = os.path.join(dirpath, filename)\n",
        "\n",
        "            # Read the entire CSV file at once\n",
        "            df = pd.read_csv(file_path, dtype=str)\n",
        "\n",
        "            # Clean column names\n",
        "            df = clean_column_names(df)\n",
        "\n",
        "            # Add a column to identify the source file\n",
        "            df['source_file'] = filename\n",
        "\n",
        "            # Convert data types\n",
        "            df = convert_dtypes(df)\n",
        "\n",
        "            # Append to the list of all DataFrames\n",
        "            all_dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "df_1962 = pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "# Print the first few rows and info about the DataFrame\n",
        "print(df_1962.head())\n",
        "print(df_1962.info())\n",
        "\n",
        "# Print column names and their data types\n",
        "print(\"\\nColumn names and data types:\")\n",
        "print(df_1962.dtypes)\n",
        "\n",
        "# Count null values in each column\n",
        "null_counts = df_1962.isnull().sum()\n",
        "\n",
        "# Print the count of null values for each column\n",
        "print(\"\\nNull value counts for each column:\")\n",
        "print(null_counts)\n",
        "\n",
        "# Calculate and print the percentage of null values in each column\n",
        "null_percentages = (null_counts / len(df_1962)) * 100\n",
        "print(\"\\nPercentage of null values in each column:\")\n",
        "print(null_percentages)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awSqnWcj0JnN"
      },
      "source": [
        "## Consolidate missing values\n",
        "We have to check if the data type conversion above was done correctly.\n",
        "If the casting was not successful, a null value gets inserted into the dataframe. You can thus check for successful conversion by determining if any null values are included in the resulting dataframe.\n",
        "\n",
        "\n",
        "> ℹ️ **Instructions** ℹ️\n",
        ">\n",
        ">Write code to compare the number of invalid entries (nulls) pre-conversion and post-conversion.\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjxNs7sb0JnN",
        "outputId": "23f59bb3-7682-418f-c6b0-a42c993e43b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Null value counts before conversion:\n",
            "date            0\n",
            "open            0\n",
            "high            0\n",
            "low            22\n",
            "close           0\n",
            "adj_close       0\n",
            "volume         21\n",
            "stock           0\n",
            "source_file     0\n",
            "dtype: int64\n",
            "\n",
            "Null value counts after conversion:\n",
            "date            0\n",
            "open            0\n",
            "high            0\n",
            "low            42\n",
            "close           0\n",
            "adj_close       0\n",
            "volume         21\n",
            "stock           0\n",
            "source_file     0\n",
            "dtype: int64\n",
            "\n",
            "Difference in null counts (after - before):\n",
            "date            0\n",
            "open            0\n",
            "high            0\n",
            "low            20\n",
            "close           0\n",
            "adj_close       0\n",
            "volume          0\n",
            "stock           0\n",
            "source_file     0\n",
            "dtype: int64\n",
            "\n",
            "Columns with increased null counts after conversion:\n",
            "['low']\n",
            "\n",
            "Data types after conversion:\n",
            "date           datetime64[ns]\n",
            "open                  float64\n",
            "high                  float64\n",
            "low                   float64\n",
            "close                 float64\n",
            "adj_close              object\n",
            "volume                float64\n",
            "stock                  object\n",
            "source_file          category\n",
            "dtype: object\n",
            "\n",
            "Sample rows with new nulls in problematic columns:\n",
            "\n",
            "Column: low\n",
            "2956      2.1733019962395237F49\n",
            "2976     -0.8318624930153021F53\n",
            "2996    -0.44164770838965717F89\n",
            "3016     0.21461880285770246F80\n",
            "3036     0.16378052416021593F70\n",
            "Name: low, dtype: object\n",
            "Converted to:\n",
            "2956   NaN\n",
            "2976   NaN\n",
            "2996   NaN\n",
            "3016   NaN\n",
            "3036   NaN\n",
            "Name: low, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "#TODO: Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Set the root directory for 1962 data\n",
        "root_dir = \"/content/drive/My Drive/1962\"\n",
        "\n",
        "# Create an empty list to store all DataFrames\n",
        "all_dfs = []\n",
        "\n",
        "# Function to clean column names\n",
        "def clean_column_names(df):\n",
        "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
        "    return df\n",
        "\n",
        "# Function to convert data types\n",
        "def convert_dtypes(df):\n",
        "    # Define the data types for each column\n",
        "    dtypes = {\n",
        "        'date': 'datetime64[ns]',\n",
        "        'symbol': 'category',\n",
        "        'open': 'float64',\n",
        "        'high': 'float64',\n",
        "        'low': 'float64',\n",
        "        'close': 'float64',\n",
        "        'volume': 'int64',\n",
        "        'source_file': 'category'\n",
        "    }\n",
        "\n",
        "    # Convert each column to its appropriate data type\n",
        "    for col, dtype in dtypes.items():\n",
        "        if col in df.columns:\n",
        "            if dtype == 'datetime64[ns]':\n",
        "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "            elif dtype in ['float64', 'int64']:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            else:\n",
        "                df[col] = df[col].astype(dtype)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Walk through the directory structure\n",
        "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "    for filename in filenames:\n",
        "        if filename.endswith('.csv'):\n",
        "            file_path = os.path.join(dirpath, filename)\n",
        "\n",
        "            # Read the entire CSV file at once\n",
        "            df = pd.read_csv(file_path, dtype=str)\n",
        "\n",
        "            # Clean column names\n",
        "            df = clean_column_names(df)\n",
        "\n",
        "            # Add a column to identify the source file\n",
        "            df['source_file'] = filename\n",
        "\n",
        "            # Append to the list of all DataFrames\n",
        "            all_dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "df_1962 = pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "# Count null values before conversion\n",
        "null_counts_before = df_1962.isnull().sum()\n",
        "\n",
        "# Convert data types\n",
        "df_1962_converted = convert_dtypes(df_1962.copy())\n",
        "\n",
        "# Count null values after conversion\n",
        "null_counts_after = df_1962_converted.isnull().sum()\n",
        "\n",
        "# Compare null counts\n",
        "print(\"Null value counts before conversion:\")\n",
        "print(null_counts_before)\n",
        "print(\"\\nNull value counts after conversion:\")\n",
        "print(null_counts_after)\n",
        "\n",
        "# Calculate and print the difference in null counts\n",
        "null_count_diff = null_counts_after - null_counts_before\n",
        "print(\"\\nDifference in null counts (after - before):\")\n",
        "print(null_count_diff)\n",
        "\n",
        "# Identify columns where null counts increased\n",
        "problematic_columns = null_count_diff[null_count_diff > 0].index.tolist()\n",
        "print(\"\\nColumns with increased null counts after conversion:\")\n",
        "print(problematic_columns)\n",
        "\n",
        "# Print data types of converted DataFrame\n",
        "print(\"\\nData types after conversion:\")\n",
        "print(df_1962_converted.dtypes)\n",
        "\n",
        "# Optional: Sample rows with new nulls in problematic columns\n",
        "if problematic_columns:\n",
        "    print(\"\\nSample rows with new nulls in problematic columns:\")\n",
        "    for col in problematic_columns:\n",
        "        mask = df_1962[col].notnull() & df_1962_converted[col].isnull()\n",
        "        if mask.any():\n",
        "            print(f\"\\nColumn: {col}\")\n",
        "            print(df_1962.loc[mask, col].head())\n",
        "            print(\"Converted to:\")\n",
        "            print(df_1962_converted.loc[mask, col].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U5UUpIp0JnN"
      },
      "source": [
        "Here you should be able to see if any of your casts went wrong.\n",
        "Do not attempt to correct any missing values at this point. This will be dealt with in later sections of the predict."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPLyyG7A0JnO"
      },
      "source": [
        "## Generate parquet files\n",
        "When writing in Spark, we typically use parquet format. This format allows parallel writing using Spark's optimisation while maintaining other useful things like metadata.\n",
        "\n",
        "When writing, it is good to make sure that the data is sufficiently partitioned.\n",
        "\n",
        "Generally, data should be partitioned with one partition for every 200MB of data, but this also depends on the size of your cluster and executors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uknl-NNI0JnO"
      },
      "source": [
        "### Check the size of the dataframe before partitioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ntn5Xpo60JnO"
      },
      "outputs": [],
      "source": [
        "from pyspark.serializers import PickleSerializer, AutoBatchedSerializer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50P-vo2y0JnO"
      },
      "outputs": [],
      "source": [
        "rdd = df.rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))\n",
        "obj = rdd.ctx._jvm.org.apache.spark.mllib.api.python.SerDe.pythonToJava(rdd._jrdd, True)\n",
        "size = sc._jvm.org.apache.spark.util.SizeEstimator.estimate(obj)\n",
        "size_MB = size/1000000\n",
        "partitions = max(int(size_MB/200), 2)\n",
        "print(f'The dataframe is {size_MB} MB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1bJg9c90JnO"
      },
      "source": [
        "### Write parquet files to the local directory\n",
        "> ℹ️ **Instructions** ℹ️\n",
        ">\n",
        "> Use the **coalesce** function and the number of **partitions** derived above to write parquet files to your local directory\n",
        ">\n",
        ">*You may use as many coding cells as necessary.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ArSrgrBn0JnO"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here\n",
        "\n",
        "!apt-get update -qq\n",
        "!apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set environment variables for Spark and Java\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\"\n",
        "\n",
        "# Initialize Spark\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit\n",
        "\n",
        "# Create Spark session with additional configuration\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Mapatha Matiba Shaun\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define root directory for 2007 data\n",
        "root_dir = \"/content/drive/My Drive/1962\"\n",
        "\n",
        "# Function to read CSV files and clean column names\n",
        "def read_and_clean_csv(file_path):\n",
        "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "    for column in df.columns:\n",
        "        df = df.withColumnRenamed(column, column.lower().replace(\" \", \"_\"))\n",
        "    return df\n",
        "\n",
        "# Read all CSV files from the directory\n",
        "df_1962 = None\n",
        "first_file = True\n",
        "\n",
        "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "    for filename in filenames:\n",
        "        if filename.endswith('.csv'):\n",
        "            file_path = os.path.join(dirpath, filename)\n",
        "            df_temp = read_and_clean_csv(file_path)\n",
        "            df_temp = df_temp.withColumn(\"source_file\", lit(filename))\n",
        "\n",
        "            # Union all DataFrames together\n",
        "            if first_file:\n",
        "                df_1962 = df_temp\n",
        "                first_file = False\n",
        "            else:\n",
        "                df_1962 = df_1962.union(df_temp)\n",
        "\n",
        "# Check if any data was loaded\n",
        "if df_1962 is None:\n",
        "    raise ValueError(\"No CSV files found in the specified directory\")\n",
        "\n",
        "# Convert data types\n",
        "df_1962 = df_1962.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "df_1962 = df_1962.withColumn(\"open\", col(\"open\").cast(\"double\"))\n",
        "df_1962 = df_1962.withColumn(\"high\", col(\"high\").cast(\"double\"))\n",
        "df_1962 = df_1962.withColumn(\"low\", col(\"low\").cast(\"double\"))\n",
        "df_1962 = df_1962.withColumn(\"close\", col(\"close\").cast(\"double\"))\n",
        "df_1962 = df_1962.withColumn(\"volume\", col(\"volume\").cast(\"long\"))\n",
        "\n",
        "# Calculate the size and number of partitions\n",
        "from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n",
        "\n",
        "rdd = df_1962.rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))\n",
        "obj = rdd.ctx._jvm.org.apache.spark.mllib.api.python.SerDe.pythonToJava(rdd._jrdd, True)\n",
        "size = spark.sparkContext._jvm.org.apache.spark.util.SizeEstimator.estimate(obj)\n",
        "size_MB = size / 1000000\n",
        "partitions = max(int(size_MB / 200), 2)\n",
        "print(f'The dataframe is {size_MB:.2f} MB')\n",
        "print(f'Number of partitions: {partitions}')\n",
        "\n",
        "# Define the output directory for parquet files\n",
        "output_dir = \"/content/drive/My Drive/1962/1962_parquet\"\n",
        "\n",
        "# Write parquet files\n",
        "df_1962.coalesce(partitions).write.mode(\"overwrite\").partitionBy(\"source_file\").parquet(output_dir)\n",
        "print(f\"Parquet files have been written to {output_dir}\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "interpreter": {
      "hash": "24a0a2ddc4dffcb168e507551dd24967ddc40ea2df7a72a200a74e0ae6d88beb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}